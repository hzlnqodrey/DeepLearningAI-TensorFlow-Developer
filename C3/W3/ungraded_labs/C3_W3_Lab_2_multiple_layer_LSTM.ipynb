{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhQDCZrKBuYm"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W3/ungraded_labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFiCyWQ-NC5D"
      },
      "source": [
        "# Ungraded Lab: Multiple LSTMs\n",
        "\n",
        "In this lab, you will look at how to build a model with multiple LSTM layers. Since you know the preceding steps already (e.g. downloading datasets, preparing the data, etc.), we won't expound on it anymore so you can just focus on the model building code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqmDNHeByJqr"
      },
      "source": [
        "## Download and Prepare the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AW-4Vo4TMUHb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Download the subword encoded pretokenized dataset\n",
        "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)\n",
        "\n",
        "# Get the tokenizer\n",
        "tokenizer = info.features['text'].encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF8bUh_5Ff7y"
      },
      "source": [
        "Like the previous lab, we increased the `BATCH_SIZE` here to make the training faster. If you are doing this on your local machine and have a powerful processor, feel free to use the value used in the lecture (i.e. 64) to get the same results as Laurence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ffvRUI0_McDS"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# Get the train and test splits\n",
        "train_data, test_data = dataset['train'], dataset['test'], \n",
        "\n",
        "# Shuffle the training data\n",
        "train_dataset = train_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "# Batch and pad the datasets to the maximum length of the sequences\n",
        "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
        "test_dataset = test_data.padded_batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcZEiG9ayNZr"
      },
      "source": [
        "## Build and Compile the Model\n",
        "\n",
        "You can build multiple layer LSTM models by simply appending another `LSTM` layer in your `Sequential` model and enabling the `return_sequences` flag to `True`. This is because an `LSTM` layer expects a sequence input so if the previous layer is also an LSTM, then it should output a sequence as well. See the code cell below that demonstrates this flag in action. You'll notice that the output dimension is in 3 dimensions `(batch_size, timesteps, features)` when when `return_sequences` is True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "18MsI2LU75kH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch_size: 1\n",
            "timesteps (sequence length): 20\n",
            "features (embedding size): 16\n",
            "lstm output units: 8\n",
            "shape of input array: (1, 20, 16)\n",
            "shape of lstm output(return_sequences=False): (1, 8)\n",
            "shape of lstm output(return_sequences=True): (1, 20, 8)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 1\n",
        "timesteps = 20\n",
        "features = 16\n",
        "lstm_dim = 8\n",
        "\n",
        "print(f'batch_size: {batch_size}')\n",
        "print(f'timesteps (sequence length): {timesteps}')\n",
        "print(f'features (embedding size): {features}')\n",
        "print(f'lstm output units: {lstm_dim}')\n",
        "\n",
        "# Define array input with random values\n",
        "random_input = np.random.rand(batch_size,timesteps,features)\n",
        "print(f'shape of input array: {random_input.shape}')\n",
        "\n",
        "# Define LSTM that returns a single output\n",
        "lstm = tf.keras.layers.LSTM(lstm_dim)\n",
        "result = lstm(random_input)\n",
        "print(f'shape of lstm output(return_sequences=False): {result.shape}')\n",
        "\n",
        "# Define LSTM that returns a sequence\n",
        "lstm_rs = tf.keras.layers.LSTM(lstm_dim, return_sequences=True)\n",
        "result = lstm_rs(random_input)\n",
        "print(f'shape of lstm output(return_sequences=True): {result.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Was3BX6_50C"
      },
      "source": [
        "The next cell implements the stacked LSTM architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VPNwU1SVyTjm"
      },
      "outputs": [
        {
          "ename": "NotImplementedError",
          "evalue": "Cannot convert a symbolic Tensor (bidirectional/forward_lstm_2/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32mc:\\.qodri-and-his-machine-learning-adventure\\2 - DeepLearningAI TensorFlow Developer\\C3\\W3\\ungraded_labs\\C3_W3_Lab_2_multiple_layer_LSTM.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/.qodri-and-his-machine-learning-adventure/2%20-%20DeepLearningAI%20TensorFlow%20Developer/C3/W3/ungraded_labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m dense_dim \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/.qodri-and-his-machine-learning-adventure/2%20-%20DeepLearningAI%20TensorFlow%20Developer/C3/W3/ungraded_labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Build the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/.qodri-and-his-machine-learning-adventure/2%20-%20DeepLearningAI%20TensorFlow%20Developer/C3/W3/ungraded_labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mSequential([\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/.qodri-and-his-machine-learning-adventure/2%20-%20DeepLearningAI%20TensorFlow%20Developer/C3/W3/ungraded_labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mEmbedding(tokenizer\u001b[39m.\u001b[39;49mvocab_size, embedding_dim),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/.qodri-and-his-machine-learning-adventure/2%20-%20DeepLearningAI%20TensorFlow%20Developer/C3/W3/ungraded_labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mBidirectional(tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mLSTM(lstm1_dim, return_sequences\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/.qodri-and-his-machine-learning-adventure/2%20-%20DeepLearningAI%20TensorFlow%20Developer/C3/W3/ungraded_labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mBidirectional(tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mLSTM(lstm2_dim)),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/.qodri-and-his-machine-learning-adventure/2%20-%20DeepLearningAI%20TensorFlow%20Developer/C3/W3/ungraded_labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mDense(dense_dim, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/.qodri-and-his-machine-learning-adventure/2%20-%20DeepLearningAI%20TensorFlow%20Developer/C3/W3/ungraded_labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mDense(\u001b[39m1\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msigmoid\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/.qodri-and-his-machine-learning-adventure/2%20-%20DeepLearningAI%20TensorFlow%20Developer/C3/W3/ungraded_labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m ])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/.qodri-and-his-machine-learning-adventure/2%20-%20DeepLearningAI%20TensorFlow%20Developer/C3/W3/ungraded_labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Print the model summary\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/.qodri-and-his-machine-learning-adventure/2%20-%20DeepLearningAI%20TensorFlow%20Developer/C3/W3/ungraded_labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:517\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 517\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    518\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:144\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[1;34m(self, layers, name)\u001b[0m\n\u001b[0;32m    142\u001b[0m   layers \u001b[39m=\u001b[39m [layers]\n\u001b[0;32m    143\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m layers:\n\u001b[1;32m--> 144\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd(layer)\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:517\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 517\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    518\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:223\u001b[0m, in \u001b[0;36mSequential.add\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_explicit_input_shape \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs:\n\u001b[0;32m    221\u001b[0m   \u001b[39m# If the model is being built continuously on top of an input layer:\u001b[39;00m\n\u001b[0;32m    222\u001b[0m   \u001b[39m# refresh its output.\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m   output_tensor \u001b[39m=\u001b[39m layer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutputs[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m    224\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(nest\u001b[39m.\u001b[39mflatten(output_tensor)) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    225\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(SINGLE_LAYER_OUTPUT_ERROR_MSG)\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py:539\u001b[0m, in \u001b[0;36mBidirectional.__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m   inputs \u001b[39m=\u001b[39m inputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m initial_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m constants \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 539\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(Bidirectional, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    541\u001b[0m \u001b[39m# Applies the same workaround as in `RNN.__call__`\u001b[39;00m\n\u001b[0;32m    542\u001b[0m additional_inputs \u001b[39m=\u001b[39m []\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:951\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[39m# Functional Model construction mode is invoked when `Layer`s are called on\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[39m# symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[0;32m    947\u001b[0m \u001b[39m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[0;32m    949\u001b[0m \u001b[39m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[0;32m    950\u001b[0m \u001b[39mif\u001b[39;00m _in_functional_construction_mode(\u001b[39mself\u001b[39m, inputs, args, kwargs, input_list):\n\u001b[1;32m--> 951\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m    952\u001b[0m                                             input_list)\n\u001b[0;32m    954\u001b[0m \u001b[39m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[0;32m    955\u001b[0m call_context \u001b[39m=\u001b[39m base_layer_utils\u001b[39m.\u001b[39mcall_context()\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1090\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1086\u001b[0m \u001b[39mif\u001b[39;00m keras_tensor\u001b[39m.\u001b[39mkeras_tensors_enabled():\n\u001b[0;32m   1087\u001b[0m   \u001b[39mwith\u001b[39;00m call_context\u001b[39m.\u001b[39menter(\n\u001b[0;32m   1088\u001b[0m       layer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, inputs\u001b[39m=\u001b[39minputs, build_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39mtraining_value):\n\u001b[0;32m   1089\u001b[0m     \u001b[39m# Check input assumptions set after layer building, e.g. input shape.\u001b[39;00m\n\u001b[1;32m-> 1090\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_keras_tensor_symbolic_call(\n\u001b[0;32m   1091\u001b[0m         inputs, input_masks, args, kwargs)\n\u001b[0;32m   1093\u001b[0m     \u001b[39mif\u001b[39;00m outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1094\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mA layer\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39ms `call` method should return a \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1095\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mTensor or a list of Tensors, not None \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1096\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39m(layer: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m).\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:822\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m   \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39mmap_structure(keras_tensor\u001b[39m.\u001b[39mKerasTensor, output_signature)\n\u001b[0;32m    821\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 822\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_output_signature(inputs, args, kwargs, input_masks)\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:863\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    857\u001b[0m   \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m    858\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m    859\u001b[0m     \u001b[39m# Build layer if applicable (if the `build` method has been\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[39m# overridden).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[39m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[39;00m\n\u001b[0;32m    862\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_build(inputs)\n\u001b[1;32m--> 863\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    865\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n\u001b[0;32m    866\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_mask_metadata(inputs, outputs, input_masks,\n\u001b[0;32m    867\u001b[0m                         build_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py:652\u001b[0m, in \u001b[0;36mBidirectional.call\u001b[1;34m(self, inputs, training, mask, initial_state, constants)\u001b[0m\n\u001b[0;32m    649\u001b[0m     forward_inputs, backward_inputs \u001b[39m=\u001b[39m inputs, inputs\n\u001b[0;32m    650\u001b[0m     forward_state, backward_state \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m   y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_layer(forward_inputs,\n\u001b[0;32m    653\u001b[0m                          initial_state\u001b[39m=\u001b[39;49mforward_state, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    654\u001b[0m   y_rev \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward_layer(backward_inputs,\n\u001b[0;32m    655\u001b[0m                               initial_state\u001b[39m=\u001b[39mbackward_state, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    656\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:660\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    654\u001b[0m inputs, initial_state, constants \u001b[39m=\u001b[39m _standardize_args(inputs,\n\u001b[0;32m    655\u001b[0m                                                      initial_state,\n\u001b[0;32m    656\u001b[0m                                                      constants,\n\u001b[0;32m    657\u001b[0m                                                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_constants)\n\u001b[0;32m    659\u001b[0m \u001b[39mif\u001b[39;00m initial_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m constants \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 660\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(RNN, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    662\u001b[0m \u001b[39m# If any of `initial_state` or `constants` are specified and are Keras\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[39m# tensors, then add them to the inputs and temporarily modify the\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[39m# input_spec to include them.\u001b[39;00m\n\u001b[0;32m    666\u001b[0m additional_inputs \u001b[39m=\u001b[39m []\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1008\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_build(inputs)\n\u001b[0;32m   1010\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1014\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1015\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py:1157\u001b[0m, in \u001b[0;36mLSTM.call\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_args_if_ragged(is_ragged_input, mask)\n\u001b[0;32m   1156\u001b[0m \u001b[39m# LSTM does not support constants. Ignore it during process.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m inputs, initial_state, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_inputs(inputs, initial_state, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m   1159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(mask, \u001b[39mlist\u001b[39m):\n\u001b[0;32m   1160\u001b[0m   mask \u001b[39m=\u001b[39m mask[\u001b[39m0\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:859\u001b[0m, in \u001b[0;36mRNN._process_inputs\u001b[1;34m(self, inputs, initial_state, constants)\u001b[0m\n\u001b[0;32m    857\u001b[0m     initial_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstates\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m initial_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 859\u001b[0m   initial_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_initial_state(inputs)\n\u001b[0;32m    861\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(initial_state) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstates):\n\u001b[0;32m    862\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mLayer has \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstates)) \u001b[39m+\u001b[39m\n\u001b[0;32m    863\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39m states but was passed \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mlen\u001b[39m(initial_state)) \u001b[39m+\u001b[39m\n\u001b[0;32m    864\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39m initial states.\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:642\u001b[0m, in \u001b[0;36mRNN.get_initial_state\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    640\u001b[0m dtype \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    641\u001b[0m \u001b[39mif\u001b[39;00m get_initial_state_fn:\n\u001b[1;32m--> 642\u001b[0m   init_state \u001b[39m=\u001b[39m get_initial_state_fn(\n\u001b[0;32m    643\u001b[0m       inputs\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49mbatch_size, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    644\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    645\u001b[0m   init_state \u001b[39m=\u001b[39m _generate_zero_filled_state(batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell\u001b[39m.\u001b[39mstate_size,\n\u001b[0;32m    646\u001b[0m                                            dtype)\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:2506\u001b[0m, in \u001b[0;36mLSTMCell.get_initial_state\u001b[1;34m(self, inputs, batch_size, dtype)\u001b[0m\n\u001b[0;32m   2505\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_initial_state\u001b[39m(\u001b[39mself\u001b[39m, inputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 2506\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_generate_zero_filled_state_for_cell(\n\u001b[0;32m   2507\u001b[0m       \u001b[39mself\u001b[39;49m, inputs, batch_size, dtype))\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:2987\u001b[0m, in \u001b[0;36m_generate_zero_filled_state_for_cell\u001b[1;34m(cell, inputs, batch_size, dtype)\u001b[0m\n\u001b[0;32m   2985\u001b[0m   batch_size \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39mshape(inputs)[\u001b[39m0\u001b[39m]\n\u001b[0;32m   2986\u001b[0m   dtype \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mdtype\n\u001b[1;32m-> 2987\u001b[0m \u001b[39mreturn\u001b[39;00m _generate_zero_filled_state(batch_size, cell\u001b[39m.\u001b[39;49mstate_size, dtype)\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:3003\u001b[0m, in \u001b[0;36m_generate_zero_filled_state\u001b[1;34m(batch_size_tensor, state_size, dtype)\u001b[0m\n\u001b[0;32m   3000\u001b[0m   \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39mzeros(init_state_size, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m   3002\u001b[0m \u001b[39mif\u001b[39;00m nest\u001b[39m.\u001b[39mis_nested(state_size):\n\u001b[1;32m-> 3003\u001b[0m   \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39;49mmap_structure(create_zeros, state_size)\n\u001b[0;32m   3004\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3005\u001b[0m   \u001b[39mreturn\u001b[39;00m create_zeros(state_size)\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:659\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    656\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    658\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 659\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    660\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:659\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    655\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    656\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    658\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 659\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    660\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:3000\u001b[0m, in \u001b[0;36m_generate_zero_filled_state.<locals>.create_zeros\u001b[1;34m(unnested_state_size)\u001b[0m\n\u001b[0;32m   2998\u001b[0m flat_dims \u001b[39m=\u001b[39m tensor_shape\u001b[39m.\u001b[39mTensorShape(unnested_state_size)\u001b[39m.\u001b[39mas_list()\n\u001b[0;32m   2999\u001b[0m init_state_size \u001b[39m=\u001b[39m [batch_size_tensor] \u001b[39m+\u001b[39m flat_dims\n\u001b[1;32m-> 3000\u001b[0m \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39;49mzeros(init_state_size, dtype\u001b[39m=\u001b[39;49mdtype)\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    202\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m    203\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    204\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2819\u001b[0m, in \u001b[0;36m_tag_zeros_tensor.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2818\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2819\u001b[0m   tensor \u001b[39m=\u001b[39m fun(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   2820\u001b[0m   tensor\u001b[39m.\u001b[39m_is_zeros_tensor \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   2821\u001b[0m   \u001b[39mreturn\u001b[39;00m tensor\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2868\u001b[0m, in \u001b[0;36mzeros\u001b[1;34m(shape, dtype, name)\u001b[0m\n\u001b[0;32m   2864\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2865\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m   2866\u001b[0m     \u001b[39m# Create a constant if it won't be very big. Otherwise create a fill\u001b[39;00m\n\u001b[0;32m   2867\u001b[0m     \u001b[39m# op to prevent serialized GraphDefs from becoming too large.\u001b[39;00m\n\u001b[1;32m-> 2868\u001b[0m     output \u001b[39m=\u001b[39m _constant_if_small(zero, shape, dtype, name)\n\u001b[0;32m   2869\u001b[0m     \u001b[39mif\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2870\u001b[0m       \u001b[39mreturn\u001b[39;00m output\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2804\u001b[0m, in \u001b[0;36m_constant_if_small\u001b[1;34m(value, shape, dtype, name)\u001b[0m\n\u001b[0;32m   2802\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_if_small\u001b[39m(value, shape, dtype, name):\n\u001b[0;32m   2803\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2804\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39;49mprod(shape) \u001b[39m<\u001b[39m \u001b[39m1000\u001b[39m:\n\u001b[0;32m   2805\u001b[0m       \u001b[39mreturn\u001b[39;00m constant(value, shape\u001b[39m=\u001b[39mshape, dtype\u001b[39m=\u001b[39mdtype, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m   2806\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   2807\u001b[0m     \u001b[39m# Happens when shape is a Tensor, list with Tensor elements, etc.\u001b[39;00m\n",
            "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mprod\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3051\u001b[0m, in \u001b[0;36mprod\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2933\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_prod_dispatcher)\n\u001b[0;32m   2934\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprod\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[0;32m   2935\u001b[0m          initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[0;32m   2936\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2937\u001b[0m \u001b[39m    Return the product of array elements over a given axis.\u001b[39;00m\n\u001b[0;32m   2938\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3049\u001b[0m \u001b[39m    10\u001b[39;00m\n\u001b[0;32m   3050\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3051\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mmultiply, \u001b[39m'\u001b[39;49m\u001b[39mprod\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, dtype, out,\n\u001b[0;32m   3052\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
            "File \u001b[1;32mc:\\Users\\HAZLAN M QODRI\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:852\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 852\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    853\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mCannot convert a symbolic Tensor (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) to a numpy array.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    854\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m This error may indicate that you\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre trying to pass a Tensor to\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    855\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m a NumPy call, which is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname))\n",
            "\u001b[1;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (bidirectional/forward_lstm_2/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 64\n",
        "lstm1_dim = 64\n",
        "lstm2_dim = 32\n",
        "dense_dim = 64\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm1_dim, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm2_dim)),\n",
        "    tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uip7QOVzMoMq"
      },
      "outputs": [],
      "source": [
        "# Set the training parameters\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh39GlZP79DY"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "The additional LSTM layer will lengthen the training time compared to the previous lab. Given the default parameters we set, it will take around 2 minutes per epoch with the Colab GPU enabled. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mlgzaRDMtF6"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mp1Z7P9pYRSK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot utility\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "# Plot the accuracy and results \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txQdN63vBlTK"
      },
      "source": [
        "## Wrap Up\n",
        "\n",
        "This lab showed how you can build deep networks by stacking LSTM layers. In the next labs, you will continue exploring other architectures you can use to implement your sentiment classification model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "C3_W3_Lab_2_multiple_layer_LSTM.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "898a950986d43450680efc03f9903704e020e6e6b23d64c62a66308a081cc53c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
